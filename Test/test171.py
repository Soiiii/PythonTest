import pandas as pd

# 주어진 데이터를 리스트로 저장
data = [
    ["Modifier and Type", "Method", "Description"],
    ["Dataset<Row>", "agg(Map<String,String> exprs)", "(Java-specific) Aggregates on the entire Dataset without groups."],
    ["Dataset<Row>", "agg(Column expr, Column... exprs)", "Aggregates on the entire Dataset without groups."],
    ["Dataset<Row>", "agg(Column expr, scala.collection.Seq<Column> exprs)", "Aggregates on the entire Dataset without groups."],
    ["Dataset<Row>", "agg(scala.collection.immutable.Map<String,String> exprs)", "(Scala-specific) Aggregates on the entire Dataset without groups."],
    ["Dataset<Row>", "agg(scala.Tuple2<String,String> aggExpr, scala.collection.Seq<scala.Tuple2<String,String>> aggExprs)", "(Scala-specific) Aggregates on the entire Dataset without groups."],
    ["Dataset<T>", "alias(String alias)", "Returns a new Dataset with an alias set."],
    ["Dataset<T>", "alias(scala.Symbol alias)", "(Scala-specific) Returns a new Dataset with an alias set."],
    ["Column", "apply(String colName)", "Selects column based on the column name and returns it as a Column."],
    ["Dataset<T>", "as(String alias)", "Returns a new Dataset with an alias set."],
    ["<U> Dataset<U>", "as(Encoder<U> evidence$2)", "Returns a new Dataset where each record has been mapped on to the specified type."],
    ["Dataset<T>", "as(scala.Symbol alias)", "(Scala-specific) Returns a new Dataset with an alias set."],
    ["Dataset<T>", "cache()", "Persist this Dataset with the default storage level (MEMORY_AND_DISK)."],
    ["Dataset<T>", "checkpoint()", "Eagerly checkpoint a Dataset and return the new Dataset."],
    ["Dataset<T>", "checkpoint(boolean eager)", "Returns a checkpointed version of this Dataset."],
    ["scala.reflect.ClassTag<T>", "classTag()", ""],
    ["Dataset<T>", "coalesce(int numPartitions)", "Returns a new Dataset that has exactly numPartitions partitions, when the fewer partitions are requested."],
    ["Column", "col(String colName)", "Selects column based on the column name and returns it as a Column."],
    ["static String", "COL_POS_KEY()", ""],
    ["Object", "collect()", "Returns an array that contains all rows in this Dataset."],
    ["List<T>", "collectAsList()", "Returns a Java list that contains all rows in this Dataset."],
    ["Column", "colRegex(String colName)", "Selects column based on the column name specified as a regex and returns it as Column."],
    ["String[]", "columns()", "Returns all column names as an array."],
    ["long", "count()", "Returns the number of rows in the Dataset."],
    ["void", "createGlobalTempView(String viewName)", "Creates a global temporary view using the given name."],
    ["void", "createOrReplaceGlobalTempView(String viewName)", "Creates or replaces a global temporary view using the given name."],
    ["void", "createOrReplaceTempView(String viewName)", "Creates a local temporary view using the given name."],
    ["void", "createTempView(String viewName)", "Creates a local temporary view using the given name."],
    ["Dataset<Row>", "crossJoin(Dataset<?> right)", "Explicit cartesian join with another DataFrame."],
    ["RelationalGroupedDataset", "cube(String col1, String... cols)", "Create a multi-dimensional cube for the current Dataset using the specified columns, so we can run aggregation on them."],
    ["RelationalGroupedDataset", "cube(String col1, scala.collection.Seq<String> cols)", "Create a multi-dimensional cube for the current Dataset using the specified columns, so we can run aggregation on them."],
    ["RelationalGroupedDataset", "cube(Column... cols)", "Create a multi-dimensional cube for the current Dataset using the specified columns, so we can run aggregation on them."],
    ["RelationalGroupedDataset", "cube(scala.collection.Seq<Column> cols)", "Create a multi-dimensional cube for the current Dataset using the specified columns, so we can run aggregation on them."],
    ["static AtomicLong", "curId()", ""],
    ["static String", "DATASET_ID_KEY()", ""],
    ["static org.apache.spark.sql.catalyst.trees.TreeNodeTag<scala.collection.mutable.HashSet<Object>>", "DATASET_ID_TAG()", ""],
    ["Dataset<Row>", "describe(String... cols)", "Computes basic statistics for numeric and string columns, including count, mean, stddev, min, and max."],
    ["Dataset<Row>", "describe(scala.collection.Seq<String> cols)", "Computes basic statistics for numeric and string columns, including count, mean, stddev, min, and max."],
    ["Dataset<T>", "distinct()", "Returns a new Dataset that contains only the unique rows from this Dataset."],
    ["Dataset<Row>", "drop(String colName)", "Returns a new Dataset with a column dropped."],
    ["Dataset<Row>", "drop(String... colNames)", "Returns a new Dataset with columns dropped."],
    ["Dataset<Row>", "drop(Column col)", "Returns a new Dataset with column dropped."],
    ["Dataset<Row>", "drop(Column col, Column... cols)", "Returns a new Dataset with columns dropped."],
    ["Dataset<Row>", "drop(Column col, scala.collection.Seq<Column> cols)", "Returns a new Dataset with columns dropped."],
    ["Dataset<Row>", "drop(scala.collection.Seq<String> colNames)", "Returns a new Dataset with columns dropped."],
    ["Dataset<T>", "dropDuplicates()", "Returns a new Dataset that contains only the unique rows from this Dataset."],
    ["Dataset<T>", "dropDuplicates(String[] colNames)", "Returns a new Dataset with duplicate rows removed, considering only the subset of columns."],
    ["Dataset<T>", "dropDuplicates(String col1, String... cols)", "Returns a new Dataset with duplicate rows removed, considering only the subset of columns."],
    ["Dataset<T>", "dropDuplicates(String col1, scala.collection.Seq<String> cols)", "Returns a new Dataset with duplicate rows removed, considering only the subset of columns."],
    ["Dataset<T>", "dropDuplicates(scala.collection.Seq<String> colNames)", "(Scala-specific) Returns a new Dataset with duplicate rows removed, considering only the subset of columns."],
    ["Dataset<T>", "dropDuplicatesWithinWatermark()", "Returns a new Dataset with duplicates rows removed, within watermark."],
    ["Dataset<T>", "dropDuplicatesWithinWatermark(String[] colNames)", "Returns a new Dataset with duplicates rows removed, considering only the subset of columns, within watermark."],
    ["Dataset<T>", "dropDuplicatesWithinWatermark(String col1, String... cols)", "Returns a new Dataset with duplicates rows removed, considering only the subset of columns, within watermark."],
    ["Dataset<T>", "dropDuplicatesWithinWatermark(String col1, scala.collection.Seq<String> cols)", "Returns a new Dataset with duplicates rows removed, considering only the subset of columns, within watermark."],
    ["Dataset<T>", "dropDuplicatesWithinWatermark(scala.collection.Seq<String> colNames)", "Returns a new Dataset with duplicates rows removed, considering only the subset of columns, within watermark."],
    ["scala.Tuple2<String,String>[]", "dtypes()", "Returns all column names and their data types as an array."],
    ["Encoder<T>", "encoder()", ""],
    ["Dataset<T>", "except(Dataset<T> other)", "Returns a new Dataset containing rows in this Dataset but not in another Dataset."],
    ["Dataset<T>", "exceptAll(Dataset<T> other)", "Returns a new Dataset containing rows in this Dataset but not in another Dataset while preserving the duplicates."],
    ["void", "explain()", "Prints the physical plan to the console for debugging purposes."],
    ["void", "explain(boolean extended)", "Prints the plans (logical and physical) to the console for debugging purposes."],
    ["void", "explain(String mode)", "Prints the plans (logical and physical) with a format specified by a given explain mode."],
    ["<A,B> Dataset<Row>", "explode(String inputColumn, String outputColumn, scala.Function1<A,scala.collection.TraversableOnce<B>> f, scala.reflect.api.TypeTags.TypeTag<B> evidence$5)", "Deprecated. use flatMap() or select() with functions.explode() instead."],
    ["<A extends scala.Product> Dataset<Row>", "explode(scala.collection.Seq<Column> input, scala.Function1<Row,scala.collection.TraversableOnce<A>> f, scala.reflect.api.TypeTags.TypeTag<A> evidence$4)", "Deprecated. use flatMap() or select() with functions.explode() instead."],
    ["Dataset<T>", "filter(String conditionExpr)", "Filters rows using the given SQL expression."],
    ["Dataset<T>", "filter(FilterFunction<T> func)", "(Java-specific) Returns a new Dataset that only contains elements where func returns true."],
    ["Dataset<T>", "filter(Column condition)", "Filters rows using the given condition."],
    ["Dataset<T>", "filter(scala.Function1<T,Object> func)", "(Scala-specific) Returns a new Dataset that only contains elements where func returns true."],
    ["T", "first()", "Returns the first row."],
    ["<U> Dataset<U>", "flatMap(FlatMapFunction<T,U> f, Encoder<U> encoder)", "(Java-specific) Returns a new Dataset by first applying a function to all elements of this Dataset, and then flattening the results."],
    ["<U> Dataset<U>", "flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> func, Encoder<U> evidence$8)", "(Scala-specific) Returns a new Dataset by first applying a function to all elements of this Dataset, and then flattening the results."],
    ["void", "foreach(ForeachFunction<T> func)", "(Java-specific) Runs func on each element of this Dataset."],
    ["void", "foreach(scala.Function1<T,scala.runtime.BoxedUnit> f)", "Applies a function f to all rows."],
    ["void", "foreachPartition(ForeachPartitionFunction<T> func)", "(Java-specific) Runs func on each partition of this Dataset."],
    ["void", "foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f)", "Applies a function f to each partition of this Dataset."],
    ["RelationalGroupedDataset", "groupBy(String col1, String... cols)", "Groups the Dataset using the specified columns, so that we can run aggregation on them."],
    ["RelationalGroupedDataset", "groupBy(String col1, scala.collection.Seq<String> cols)", "Groups the Dataset using the specified columns, so that we can run aggregation on them."],
    ["RelationalGroupedDataset", "groupBy(Column... cols)", "Groups the Dataset using the specified columns, so we can run aggregation on them."],
    ["RelationalGroupedDataset", "groupBy(scala.collection.Seq<Column> cols)", "Groups the Dataset using the specified columns, so we can run aggregation on them."],
    ["<K> KeyValueGroupedDataset<K,T>", "groupByKey(MapFunction<T,K> func, Encoder<K> encoder)", "(Java-specific) Returns a KeyValueGroupedDataset where the data is grouped by the given key func."],
    ["<K> KeyValueGroupedDataset<K,T>", "groupByKey(scala.Function1<T,K> func, Encoder<K> evidence$3)", "(Scala-specific) Returns a KeyValueGroupedDataset where the data is grouped by the given key func."],
    ["T", "head()", "Returns the first row."],
    ["Object", "head(int n)", "Returns the first n rows."],
    ["Dataset<T>", "hint(String name, Object... parameters)", "Specifies some hint on the current Dataset."],
    ["Dataset<T>", "hint(String name, scala.collection.Seq<Object> parameters)", "Specifies some hint on the current Dataset."],
    ["String[]", "inputFiles()", "Returns a best-effort snapshot of the files that compose this Dataset."],
    ["Dataset<T>", "intersect(Dataset<T> other)", "Returns a new Dataset containing rows only in both this Dataset and another Dataset."],
    ["Dataset<T>", "intersectAll(Dataset<T> other)", "Returns a new Dataset containing rows only in both this Dataset and another Dataset while preserving the duplicates."],
    ["boolean", "isEmpty()", "Returns true if the Dataset is empty."],
    ["boolean", "isLocal()", "Returns true if the collect and take methods can be run locally (without any Spark executors)."],
    ["boolean", "isStreaming()", "Returns true if this Dataset contains one or more sources that continuously return data as it arrives."],
    ["JavaRDD<T>", "javaRDD()", "Returns the content of the Dataset as a JavaRDD of Ts."],
    ["Dataset<Row>", "join(Dataset<?> right)", "Join with another DataFrame."],
    ["Dataset<Row>", "join(Dataset<?> right, String usingColumn)", "Inner equi-join with another DataFrame using the given column."],
    ["Dataset<Row>", "join(Dataset<?> right, String[] usingColumns)", "(Java-specific) Inner equi-join with another DataFrame using the given columns."],
    ["Dataset<Row>", "join(Dataset<?> right, String[] usingColumns, String joinType)", "(Java-specific) Equi-join with another DataFrame using the given columns."],
    ["Dataset<Row>", "join(Dataset<?> right, String usingColumn, String joinType)", "Equi-join with another DataFrame using the given column."],
    ["Dataset<Row>", "join(Dataset<?> right, Column joinExprs)", "Inner join with another DataFrame, using the given join expression."],
    ["Dataset<Row>", "join(Dataset<?> right, Column joinExprs, String joinType)", "Join with another DataFrame, using the given join expression."],
    ["Dataset<Row>", "join(Dataset<?> right, scala.collection.Seq<String> usingColumns)", "(Scala-specific) Inner equi-join with another DataFrame using the given columns."],
    ["Dataset<Row>", "join(Dataset<?> right, scala.collection.Seq<String> usingColumns, String joinType)", "(Scala-specific) Equi-join with another DataFrame using the given columns."],
    ["<U> Dataset<scala.Tuple2<T,U>>", "joinWith(Dataset<U> other, Column condition)", "Using inner equi-join to join this Dataset returning a Tuple2 for each pair where condition evaluates to true."],
    ["<U> Dataset<scala.Tuple2<T,U>>", "joinWith(Dataset<U> other, Column condition, String joinType)", "Joins this Dataset returning a Tuple2 for each pair where condition evaluates to true."],
    ["Dataset<T>", "limit(int n)", "Returns a new Dataset by taking the first n rows."],
    ["Dataset<T>", "localCheckpoint()", "Eagerly locally checkpoints a Dataset and return the new Dataset."],
    ["Dataset<T>", "localCheckpoint(boolean eager)", "Locally checkpoints a Dataset and return the new Dataset."],
    ["<U> Dataset<U>", "map(MapFunction<T,U> func, Encoder<U> encoder)", "(Java-specific) Returns a new Dataset that contains the result of applying func to each element."],
    ["<U> Dataset<U>", "map(scala.Function1<T,U> func, Encoder<U> evidence$6)", "(Scala-specific) Returns a new Dataset that contains the result of applying func to each element."],
    ["<U> Dataset<U>", "mapPartitions(MapPartitionsFunction<T,U> f, Encoder<U> encoder)", "(Java-specific) Returns a new Dataset that contains the result of applying f to each partition."],
    ["<U> Dataset<U>", "mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> func, Encoder<U> evidence$7)", "(Scala-specific) Returns a new Dataset that contains the result of applying func to each partition."],
    ["Dataset<Row>", "melt(Column[] ids, String variableColumnName, String valueColumnName)", "Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set."],
    ["Dataset<Row>", "melt(Column[] ids, Column[] values, String variableColumnName, String valueColumnName)", "Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set."],
    ["Column", "metadataColumn(String colName)", "Selects a metadata column based on its logical column name, and returns it as a Column."],
    ["DataFrameNaFunctions", "na()", "Returns a DataFrameNaFunctions for working with missing data."],
    ["Dataset<T>", "observe(String name, Column expr, Column... exprs)", "Define (named) metrics to observe on the Dataset."],
    ["Dataset<T>", "observe(String name, Column expr, scala.collection.Seq<Column> exprs)", "Define (named) metrics to observe on the Dataset."],
    ["Dataset<T>", "observe(Observation observation, Column expr, Column... exprs)", "Observe (named) metrics through an org.apache.spark.sql.Observation instance."],
    ["Dataset<T>", "observe(Observation observation, Column expr, scala.collection.Seq<Column> exprs)", "Observe (named) metrics through an org.apache.spark.sql.Observation instance."],
    ["Dataset<T>", "offset(int n)", "Returns a new Dataset by skipping the first n rows."],
    ["static Dataset<Row>", "ofRows(SparkSession sparkSession, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan logicalPlan)", ""],
    ["static Dataset<Row>", "ofRows(SparkSession sparkSession, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan logicalPlan, org.apache.spark.sql.catalyst.QueryPlanningTracker tracker)", "A variant of ofRows that allows passing in a tracker so we can track query parsing time."],
    ["Dataset<T>", "orderBy(String sortCol, String... sortCols)", "Returns a new Dataset sorted by the given expressions."],
    ["Dataset<T>", "orderBy(String sortCol, scala.collection.Seq<String> sortCols)", "Returns a new Dataset sorted by the given expressions."],
    ["Dataset<T>", "orderBy(Column... sortExprs)", "Returns a new Dataset sorted by the given expressions."],
    ["Dataset<T>", "orderBy(scala.collection.Seq<Column> sortExprs)", "Returns a new Dataset sorted by the given expressions."],
    ["Dataset<T>", "persist()", "Persist this Dataset with the default storage level (MEMORY_AND_DISK)."],
    ["Dataset<T>", "persist(StorageLevel newLevel)", "Persist this Dataset with the given storage level."],
    ["void", "printSchema()", "Prints the schema to the console in a nice tree format."],
    ["void", "printSchema(int level)", "Prints the schema up to the given level to the console in a nice tree format."],
    ["org.apache.spark.sql.execution.QueryExecution", "queryExecution()", ""],
    ["Dataset<T>[]", "randomSplit(double[] weights)", "Randomly splits this Dataset with the provided weights."],
    ["Dataset<T>[]", "randomSplit(double[] weights, long seed)", "Randomly splits this Dataset with the provided weights."],
    ["List<Dataset<T>>", "randomSplitAsList(double[] weights, long seed)", "Returns a Java list that contains randomly split Dataset with the provided weights."],
    ["RDD<T>", "rdd()", ""],
    ["T", "reduce(ReduceFunction<T> func)", "(Java-specific) Reduces the elements of this Dataset using the specified binary function."],
    ["T", "reduce(scala.Function2<T,T,T> func)", "(Scala-specific) Reduces the elements of this Dataset using the specified binary function."],
    ["void", "registerTempTable(String tableName)", "Deprecated. Use createOrReplaceTempView(viewName) instead."],
    ["Dataset<T>", "repartition(int numPartitions)", "Returns a new Dataset that has exactly numPartitions partitions."],
    ["Dataset<T>", "repartition(int numPartitions, Column... partitionExprs)", "Returns a new Dataset partitioned by the given partitioning expressions into numPartitions."],
    ["Dataset<T>", "repartition(int numPartitions, scala.collection.Seq<Column> partitionExprs)", "Returns a new Dataset partitioned by the given partitioning expressions into numPartitions."],
    ["Dataset<T>", "repartition(Column... partitionExprs)", "Returns a new Dataset partitioned by the given partitioning expressions, using spark.sql.shuffle.partitions as number of partitions."],
    ["Dataset<T>", "repartition(scala.collection.Seq<Column> partitionExprs)", "Returns a new Dataset partitioned by the given partitioning expressions, using spark.sql.shuffle.partitions as number of partitions."],
    ["Dataset<T>", "repartitionByRange(int numPartitions, Column... partitionExprs)", "Returns a new Dataset partitioned by the given partitioning expressions into numPartitions."],
    ["Dataset<T>", "repartitionByRange(int numPartitions, scala.collection.Seq<Column> partitionExprs)", "Returns a new Dataset partitioned by the given partitioning expressions into numPartitions."],
    ["Dataset<T>", "repartitionByRange(Column... partitionExprs)", "Returns a new Dataset partitioned by the given partitioning expressions, using spark.sql.shuffle.partitions as number of partitions."],
    ["Dataset<T>", "repartitionByRange(scala.collection.Seq<Column> partitionExprs)", "Returns a new Dataset partitioned by the given partitioning expressions, using spark.sql.shuffle.partitions as number of partitions."],
    ["RelationalGroupedDataset", "rollup(String col1, String... cols)", "Create a multi-dimensional rollup for the current Dataset using the specified columns, so we can run aggregation on them."],
    ["RelationalGroupedDataset", "rollup(String col1, scala.collection.Seq<String> cols)", "Create a multi-dimensional rollup for the current Dataset using the specified columns, so we can run aggregation on them."],
    ["RelationalGroupedDataset", "rollup(Column... cols)", "Create a multi-dimensional rollup for the current Dataset using the specified columns, so we can run aggregation on them."],
    ["RelationalGroupedDataset", "rollup(scala.collection.Seq<Column> cols)", "Create a multi-dimensional rollup for the current Dataset using the specified columns, so we can run aggregation on them."],
    ["boolean", "sameSemantics(Dataset<T> other)", "Returns true when the logical query plans inside both Datasets are equal and therefore return same results."],
    ["Dataset<T>", "sample(boolean withReplacement, double fraction)", "Returns a new Dataset by sampling a fraction of rows, using a random seed."],
    ["Dataset<T>", "sample(boolean withReplacement, double fraction, long seed)", "Returns a new Dataset by sampling a fraction of rows, using a user-supplied seed."],
    ["Dataset<T>", "sample(double fraction)", "Returns a new Dataset by sampling a fraction of rows (without replacement), using a random seed."],
    ["Dataset<T>", "sample(double fraction, long seed)", "Returns a new Dataset by sampling a fraction of rows (without replacement), using a user-supplied seed."],
    ["StructType", "schema()", "Returns the schema of this Dataset."],
    ["Dataset<Row>", "select(String col, String... cols)", "Selects a set of columns."],
    ["Dataset<Row>", "select(String col, scala.collection.Seq<String> cols)", "Selects a set of columns."],
    ["Dataset<Row>", "select(Column... cols)", "Selects a set of column based expressions."],
    ["<U1> Dataset<U1>", "select(TypedColumn<T,U1> c1)", "Returns a new Dataset by computing the given Column expression for each element."],
    ["<U1,U2> Dataset<scala.Tuple2<U1,U2>>", "select(TypedColumn<T,U1> c1, TypedColumn<T,U2> c2)", "Returns a new Dataset by computing the given Column expressions for each element."],
    ["<U1,U2,U3> Dataset<scala.Tuple3<U1,U2,U3>>", "select(TypedColumn<T,U1> c1, TypedColumn<T,U2> c2, TypedColumn<T,U3> c3)", "Returns a new Dataset by computing the given Column expressions for each element."],
    ["<U1,U2,U3,U4> Dataset<scala.Tuple4<U1,U2,U3,U4>>", "select(TypedColumn<T,U1> c1, TypedColumn<T,U2> c2, TypedColumn<T,U3> c3, TypedColumn<T,U4> c4)", "Returns a new Dataset by computing the given Column expressions for each element."],
    ["<U1,U2,U3,U4,U5> Dataset<scala.Tuple5<U1,U2,U3,U4,U5>>", "select(TypedColumn<T,U1> c1, TypedColumn<T,U2> c2, TypedColumn<T,U3> c3, TypedColumn<T,U4> c4, TypedColumn<T,U5> c5)", "Returns a new Dataset by computing the given Column expressions for each element."],
    ["Dataset<Row>", "select(scala.collection.Seq<Column> cols)", "Selects a set of column based expressions."],
    ["Dataset<Row>", "selectExpr(String... exprs)", "Selects a set of SQL expressions."],
    ["Dataset<Row>", "selectExpr(scala.collection.Seq<String> exprs)", "Selects a set of SQL expressions."],
    ["int", "semanticHash()", "Returns a hashCode of the logical query plan against this Dataset."],
    ["void", "show()", "Displays the top 20 rows of Dataset in a tabular form."],
    ["void", "show(boolean truncate)", "Displays the top 20 rows of Dataset in a tabular form."],
    ["void", "show(int numRows)", "Displays the Dataset in a tabular form."],
    ["void", "show(int numRows, boolean truncate)", "Displays the Dataset in a tabular form."],
    ["void", "show(int numRows, int truncate)", "Displays the Dataset in a tabular form."],
    ["void", "show(int numRows, int truncate, boolean vertical)", "Displays the Dataset in a tabular form."],
    ["Dataset<T>", "sort(String sortCol, String... sortCols)", "Returns a new Dataset sorted by the specified column, all in ascending order."],
    ["Dataset<T>", "sort(String sortCol, scala.collection.Seq<String> sortCols)", "Returns a new Dataset sorted by the specified column, all in ascending order."],
    ["Dataset<T>", "sort(Column... sortExprs)", "Returns a new Dataset sorted by the given expressions."],
    ["Dataset<T>", "sort(scala.collection.Seq<Column> sortExprs)", "Returns a new Dataset sorted by the given expressions."],
    ["Dataset<T>", "sortWithinPartitions(String sortCol, String... sortCols)", "Returns a new Dataset with each partition sorted by the given expressions."],
    ["Dataset<T>", "sortWithinPartitions(String sortCol, scala.collection.Seq<String> sortCols)", "Returns a new Dataset with each partition sorted by the given expressions."],
    ["Dataset<T>", "sortWithinPartitions(Column... sortExprs)", "Returns a new Dataset with each partition sorted by the given expressions."],
    ["Dataset<T>", "sortWithinPartitions(scala.collection.Seq<Column> sortExprs)", "Returns a new Dataset with each partition sorted by the given expressions."],
    ["SparkSession", "sparkSession()", ""],
    ["SQLContext", "sqlContext()", ""],
    ["DataFrameStatFunctions", "stat()", "Returns a DataFrameStatFunctions for working statistic functions support."],
    ["StorageLevel", "storageLevel()", "Get the Dataset's current storage level, or StorageLevel.NONE if not persisted."],
    ["Dataset<Row>", "summary(String... statistics)", "Computes specified statistics for numeric and string columns."],
    ["Dataset<Row>", "summary(scala.collection.Seq<String> statistics)", "Computes specified statistics for numeric and string columns."],
    ["Object", "tail(int n)", "Returns the last n rows in the Dataset."],
    ["Object", "take(int n)", "Returns the first n rows in the Dataset."],
    ["List<T>", "takeAsList(int n)", "Returns the first n rows in the Dataset as a list."],
    ["Dataset<Row>", "to(StructType schema)", "Returns a new DataFrame where each row is reconciled to match the specified schema."],
    ["Dataset<Row>", "toDF()", "Converts this strongly typed collection of data to generic Dataframe."],
    ["Dataset<Row>", "toDF(String... colNames)", "Converts this strongly typed collection of data to generic DataFrame with columns renamed."],
    ["Dataset<Row>", "toDF(scala.collection.Seq<String> colNames)", "Converts this strongly typed collection of data to generic DataFrame with columns renamed."],
    ["JavaRDD<T>", "toJavaRDD()", "Returns the content of the Dataset as a JavaRDD of Ts."],
    ["Dataset<String>", "toJSON()", "Returns the content of the Dataset as a Dataset of JSON strings."],
    ["Iterator<T>", "toLocalIterator()", "Returns an iterator that contains all rows in this Dataset."],
    ["String", "toString()", ""],
    ["<U> Dataset<U>", "transform(scala.Function1<Dataset<T>,Dataset<U>> t)", "Concise syntax for chaining custom transformations."],
    ["Dataset<T>", "union(Dataset<T> other)", "Returns a new Dataset containing union of rows in this Dataset and another Dataset."],
    ["Dataset<T>", "unionAll(Dataset<T> other)", "Returns a new Dataset containing union of rows in this Dataset and another Dataset."],
    ["Dataset<T>", "unionByName(Dataset<T> other)", "Returns a new Dataset containing union of rows in this Dataset and another Dataset."],
    ["Dataset<T>", "unionByName(Dataset<T> other, boolean allowMissingColumns)", "Returns a new Dataset containing union of rows in this Dataset and another Dataset."],
    ["Dataset<T>", "unpersist()", "Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk."],
    ["Dataset<T>", "unpersist(boolean blocking)", "Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk."],
    ["Dataset<Row>", "unpivot(Column[] ids, String variableColumnName, String valueColumnName)", "Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set."],
    ["Dataset<Row>", "unpivot(Column[] ids, Column[] values, String variableColumnName, String valueColumnName)", "Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set."],
    ["Dataset<T>", "where(String conditionExpr)", "Filters rows using the given SQL expression."],
    ["Dataset<T>", "where(Column condition)", "Filters rows using the given condition."],
    ["Dataset<Row>", "withColumn(String colName, Column col)", "Returns a new Dataset by adding a column or replacing the existing column that has the same name."],
    ["Dataset<Row>", "withColumnRenamed(String existingName, String newName)", "Returns a new Dataset with a column renamed."],
    ["Dataset<Row>", "withColumns(Map<String,Column> colsMap)", "(Java-specific) Returns a new Dataset by adding columns or replacing the existing columns that has the same names."],
    ["Dataset<Row>", "withColumns(scala.collection.immutable.Map<String,Column> colsMap)", "(Scala-specific) Returns a new Dataset by adding columns or replacing the existing columns that has the same names."],
    ["Dataset<Row>", "withColumnsRenamed(Map<String,String> colsMap)", "(Java-specific) Returns a new Dataset with a columns renamed."],
    ["Dataset<Row>", "withColumnsRenamed(scala.collection.immutable.Map<String,String> colsMap)", "(Scala-specific) Returns a new Dataset with a columns renamed."],
    ["Dataset<Row>", "withMetadata(String columnName, Metadata metadata)", "Returns a new Dataset by updating an existing column with metadata."],
    ["Dataset<T>", "withWatermark(String eventTime, String delayThreshold)", "Defines an event time watermark for this Dataset."],
    ["DataFrameWriter<T>", "write()", "Interface for saving the content of the non-streaming Dataset out into external storage."],
    ["DataStreamWriter<T>", "writeStream()", "Interface for saving the content of the streaming Dataset out into external storage."],
    ["DataFrameWriterV2<T>", "writeTo(String table)", "Create a write configuration builder for v2 sources."]
]

# 데이터를 DataFrame으로 변환
df = pd.DataFrame(data[1:], columns=data[0])

# CSV 파일로 저장
df.to_csv("method_summary.csv", index=False)

print("CSV 파일이 성공적으로 생성되었습니다.")

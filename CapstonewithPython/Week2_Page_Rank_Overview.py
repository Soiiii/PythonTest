# Page Rank
# Write a simple web page crawler
# Compute a simple version of Google's Page Rank algorithm
# Visualize the resulting network

# Web Crawler
# A web crawler is a computer program that browses the World Wide Web in a
# methodical, automated manner. Web crawlers are mainly used to create a copy of all the
# visited pages for later processing by a search engine that will index the downloaded pages to provide
# fast searches

# Web Crawler
# Retrieve a page
# Look through the page for links
# Add the links to a list of "to be retrieved" sites

#Web Crawling Policy
# a selection policy that states which pages to downlaod,
# a re-visit policy that states when to check for changes to the pages,
# a politeness policy that states how to avoid overloading Web sites, and
# a parallelization policy that states how to coordinate distributed Web crawlers

# robots.txt
# A way for a web site to communicate with web crawlers
# An informal and voluntary standard
# Sometimes folks make a "Spide Trap" to catch "bad" spiders
